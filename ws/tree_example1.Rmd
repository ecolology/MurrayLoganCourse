---
title: "Regression Trees Part1"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../resources/style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# Preparations

https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2008.01390.x

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE}
library(gbm)         #for gradient boosted models
library(car)
library(dismo)
library(pdp)
library(ggfortify)
library(randomForest)
library(tidyverse)
library(gridExtra)
library(patchwork)
theme_set(theme_classic())
```

# Scenario

Abalone are an important aquaculture shell fish that are farmed
for both their meat and their shells.  Abalone can live up to 50 
years, although their longevity is known to be influenced by a 
range of environmental factors.  Traditionally, abalone are aged
by counting thier growth rings, however, this method is very
laborious and expensive.  Hence a study was conducted in which abalone
growth ring counts were matched up with a range of other more easily
measured physical characteristics (such as shell dimensions and weights)
in order to see if any of these other parameters could be used as
proxies for the number of growth rings (or age).

![abalone](../resources/abalone.jpg){width="251" height="290"}

Format of abalone.csv data file


# Read in the data

```{r readData, results='markdown', eval=TRUE}
abalone <- read_csv('../data/abalone.csv', trim_ws=TRUE) %>%
  janitor::clean_names() %>%
  mutate(sex=factor(sex))
glimpse(abalone)
```
# Explanation of boosted regression trees:
Boosting: Learn via constructing extremely small regression trees from limited data while limiting over-learning to make extremely accurate predictions. But how do we tell it when to stop learning? Use squared error loss to determine best number of splits.

validation set: 75% test-25% train
out of bag: 50-50%
cross validation: 3-fold


# Exploratory data analysis
```{r}
car::scatterplotMatrix(~rings + sex + length + diameter + height + whole_weight + meat_weight + gut_weight + shell_weight, 
                  data=abalone, diagonal = list(method = 'boxplot'),
                  regLine = list(col="red"))
```

Clear correlations across weights

# Fit the model
First we will obtain train and test sets.
To ensure we all have the same folds, we use seed=123.
```{r}
set.seed(123)
i <- sample(1:nrow(abalone), size=100, replace = FALSE)
abalone_train <- abalone[-i,]
abalone_test <- abalone[i,]
```

Next, run a boosted regression tree on the train dataset:
```{r}
abalone_brt <- gbm(rings ~ sex + length + diameter + height + whole_weight + meat_weight + gut_weight + shell_weight,
                   data = abalone_train,
                   distribution = "poisson", # count of rings
                   var.monotone = c(0,1,1,1,1,1,1,1), # forces what the type of relationship with the predictor is to be (mostly positive in this case, but sex is not helpful)
                   n.trees = 1e4,
                   interaction.depth = 5,
                   bag.fraction = 0.5, 
                   shrinkage = 0.01,
                   train.fraction = 1, # training fraction for tree pruning is 100%, and should always be set to 1, as out of bag is far better anyways
                   cv.folds = 3) # number of trees
```
Note that the function uses a distribution, rather than a family, as the gbm algorithm works on a loss function, rather than an optimization of likelihood.

To deal with complex interactions among variables, we set interaction.depth = 5.

Shrinkage deals with the fact that trees learn 1% of all previous knowledge

Ideally, we want to get the optimal number of iterations it ran for using:
```{r}
(best.iter <- gbm.perf(abalone_brt, method = "OOB"))
(best.iter <- gbm.perf(abalone_brt, method = "cv"))
```
This plot is looking for where the curves bottom out, as the optimum number of iterations. The blue dashed line is the optimal learning, which occurred much too early in this model, clearly! Ideally it would bottom out around 1000-2000 trees. Thus the model is learning too rapidly, and thus we need to slow learning down through the shrinkage parameter.

```{r}
abalone_brt <- gbm(rings ~ sex + length + diameter + height + whole_weight + meat_weight + gut_weight + shell_weight,
                   data = abalone_train,
                   distribution = "poisson", # count of rings
                   var.monotone = c(0,1,1,1,1,1,1,1), # forces what the type of relationship with the predictor is to be (mostly positive in this case, but sex is not helpful)
                   n.trees = 1e4,
                   interaction.depth = 5,
                   bag.fraction = 0.5, 
                   shrinkage = 0.003, # a.k.a. learning rate, determines how much a single tree informs the larger model 
                   train.fraction = 1, # training fraction for tree pruning is 100%, and should always be set to 1, as out of bag is far better anyways
                   cv.folds = 3) # number of trees
```

Determine best number of iterations:
```{r}
gbm.perf(abalone_brt, method = "OOB")
(best.iter <- gbm.perf(abalone_brt, method = "cv"))
```
Between 1000-2000, so looks good!


```{r}
# find index for n trees with minimum CV error
min_MSE <- which.min(abalone_brt$cv.error)

# get Mean squared error (no Root-MSE for negative poisson deviances)
abalone_brt$cv.error[min_MSE]
```


# Explore relative influence

```{r}
summary(abalone_brt, n.trees = best.iter)
```

Shell weight is by far the most important variable (86.5% relative influence), while height (5.9%), sex (4.8%), and diameter (1.6%) also have weak predictive power. Everything else is less than 1% relative influence.

**The cut-off for influential-ness is 1/number of variables**, since this would be what is considered 'substantially influential' over the other variables. 
```{r}
(brt_terms <- attr(abalone_brt$Terms, "term.labels"))
```

In this case, we have 8 terms, so they have to be above `r 1/8*100`%.

# Explore partial effects
```{r}
plot(abalone_brt, 8, n.tree = best.iter)

abalone_brt %>%
  pdp::partial(pred.var = "shell_weight",
               n.trees = best.iter,
               recursive = F, # speeds up calculations
               inv.link = exp) %>%
  autoplot()
```

```{r}
abalone_brt %>%
  pdp::partial(pred.var = c("shell_weight", "height"),
               n.trees = best.iter,
               recursive = T) %>%
  autoplot()
```

```{r}
interact.gbm(abalone_brt, abalone, c("shell_weight", "height"), 
             n.tree = best.iter)
```
This reported coefficient indicates 0 for no interaction, whereas 1 is a perfect interaction.

Note: we only do this on variables considered influential!


# Explore accuracy
```{r}
abalone_acc <- 
  abalone_test %>%
  bind_cols(pred = predict(abalone_brt, 
            newdata = abalone_test,
            n.tree = best.iter,
            type = "response"))

head(abalone_acc)

abalone_acc %>%
  ggplot() +
  geom_point(aes(y = pred, x = rings))
```


# Explore interactions {.tabset .tabset-faded}


# Tuning
Could try different values and loop over to determine the uncertainty with different tuning.

From: http://uc-r.github.io/gbm_regression#gbm

```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .005, 0.003, 0.001),
  interaction.depth = c(1, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.7, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_MSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid)
## [1] 72



for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm_tune <- with(hyper_grid, gbm(rings ~ sex + length + diameter + height +
                                    whole_weight + meat_weight + 
                                    gut_weight + shell_weight,
                  data = abalone_train,
                  distribution = "poisson", # count of rings
                  var.monotone = c(0,1,1,1,1,1,1,1), # forces what the type of relationship with the predictor is to be (mostly positive in this case, but sex is not helpful)
                  n.trees = 1e4,
                  interaction.depth = interaction.depth[i],
                  shrinkage = shrinkage[i],
                  n.minobsinnode = n.minobsinnode[i],
                  bag.fraction = bag.fraction[i],
                  train.fraction = 1, # training fraction for tree pruning is 100%, and should always be set to 1, as out of bag is far better anyways
                  cv.folds = 3,
                  verbose = FALSE))
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm_tune$valid.error)
  hyper_grid$min_MSE[i] <- min(gbm_tune$valid.error)
}

hyper_grid %>% 
  dplyr::arrange(min_MSE) %>%
  pull(min_MSE)
  head(10)
```

We would then fit the final model using the best parameterization with the optimal number of trees.





# Random Forests

Difference between random forest and BRTs: boosted regression trees determine the single, optimal influences, whereas random forests would find that all variables are similar.

```{r}
library(randomForest)
abalone_rf <- randomForest(rings ~ sex + length + diameter + height +
                      whole_weight + meat_weight + gut_weight + shell_weight,
                      data=abalone, importance=TRUE,
                      ntree=1000)
abalone_imp <- randomForest::importance(abalone_rf)
## Rank by either:
## *MSE (mean decrease in accuracy)
## For each tree, calculate OOB prediction error.
## This also done after permuting predictors.
## Then average diff of prediction errors for each tree
## *NodePurity (mean decrease in node impurity)
## Measure of the total decline of impurity due to each
## predictor averaged over trees
100*abalone_imp/sum(abalone_imp)
varImpPlot(abalone_rf)
## use brute force
abalone_rf %>% pdp::partial('SHELL_WEIGHT') %>% autoplot
```
